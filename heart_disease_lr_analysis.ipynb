{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e5cc2b8",
   "metadata": {},
   "source": [
    "# Heart Disease Risk Prediction: Logistic Regression Homework\n",
    "\n",
    "**Student Name:** [Tu Nombre]  \n",
    "**Date:** January 28, 2026\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Heart disease is the world's leading cause of death, claiming approximately 18 million lives each year, as reported by the World Health Organization. Predictive models like logistic regression can enable early identification of at-risk patients by analyzing clinical features such as age, cholesterol, and blood pressure.\n",
    "\n",
    "This notebook implements **logistic regression** on the **Heart Disease Dataset** from Kaggle—a real-world UCI repository collection of 303 patient records with 14 features and a binary target (1 for disease presence, 0 for absence).\n",
    "\n",
    "### Dataset Source\n",
    "- **Kaggle:** https://www.kaggle.com/datasets/neurocipher/heartdisease\n",
    "- **Description:** 303 patient records with 14 clinical features\n",
    "- **Target:** Heart Disease (Presence/Absence)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b828c9bd",
   "metadata": {},
   "source": [
    "# Step 1: Load and Prepare the Dataset\n",
    "\n",
    "In this section, we will:\n",
    "1. Load the Heart Disease dataset from CSV\n",
    "2. Perform Exploratory Data Analysis (EDA)\n",
    "3. Handle missing values and outliers\n",
    "4. Binarize the target variable\n",
    "5. Select relevant features\n",
    "6. Split data into train/test sets (70/30, stratified)\n",
    "7. Normalize numerical features\n",
    "\n",
    "---\n",
    "\n",
    "## 1.1 Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e473fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddebb918",
   "metadata": {},
   "source": [
    "## 1.2 Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f43f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Heart Disease dataset\n",
    "df = pd.read_csv('Heart_Disease_Prediction.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"\\nDataset Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"First 5 rows:\")\n",
    "print(\"=\"*60)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48ce867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all column names\n",
    "print(\"Column Names:\")\n",
    "print(\"=\"*60)\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2c17b8",
   "metadata": {},
   "source": [
    "## 1.3 Initial Data Exploration\n",
    "\n",
    "Let's examine the dataset structure, data types, and summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5085f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset info\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\"*60)\n",
    "df.info()\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Summary Statistics:\")\n",
    "print(\"=\"*60)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c804aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in target column\n",
    "print(\"Target Variable: 'Heart Disease'\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Unique values: {df['Heart Disease'].unique()}\")\n",
    "print(f\"\\nValue counts:\")\n",
    "print(df['Heart Disease'].value_counts())\n",
    "print(f\"\\nValue counts (%):\")\n",
    "print(df['Heart Disease'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a04761",
   "metadata": {},
   "source": [
    "### Binarize the Target Variable\n",
    "\n",
    "We need to convert the target variable from \"Presence\"/\"Absence\" to binary (1/0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3cda16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize target: 1 = Presence (disease), 0 = Absence (no disease)\n",
    "df['Target'] = (df['Heart Disease'] == 'Presence').astype(int)\n",
    "\n",
    "# Verify the mapping\n",
    "print(\"Target Variable Binarization:\")\n",
    "print(\"=\"*60)\n",
    "print(\"Original -> Binarized:\")\n",
    "print(df[['Heart Disease', 'Target']].drop_duplicates())\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df['Target'].value_counts().sort_index())\n",
    "print(f\"\\nDisease prevalence: {df['Target'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226828d5",
   "metadata": {},
   "source": [
    "## 1.4 Handle Missing Values and Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699ef6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "print(\"=\"*60)\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() == 0:\n",
    "    print(\"✓ No missing values found in the dataset!\")\n",
    "else:\n",
    "    print(missing[missing > 0])\n",
    "    print(f\"\\nTotal missing: {missing.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f7680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify numerical columns (excluding target and categorical)\n",
    "numerical_cols = ['Age', 'BP', 'Cholesterol', 'Max HR', 'ST depression']\n",
    "\n",
    "# Visualize potential outliers using box plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numerical_cols):\n",
    "    if idx < len(axes):\n",
    "        axes[idx].boxplot(df[col].dropna(), vert=True)\n",
    "        axes[idx].set_title(f'{col} Distribution')\n",
    "        axes[idx].set_ylabel('Value')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Remove empty subplot\n",
    "if len(numerical_cols) < len(axes):\n",
    "    fig.delaxes(axes[-1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Box Plots for Outlier Detection', y=1.02, fontsize=14, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Outlier Analysis (using IQR method):\")\n",
    "print(\"=\"*60)\n",
    "for col in numerical_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    print(f\"{col:20s}: {len(outliers):3d} outliers ({len(outliers)/len(df)*100:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4357d8e0",
   "metadata": {},
   "source": [
    "**Note:** We will keep the outliers as they may represent valid extreme clinical cases. Medical data often contains legitimate extreme values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9055426",
   "metadata": {},
   "source": [
    "## 1.5 Visualize Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b074e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations for target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "target_counts = df['Target'].value_counts().sort_index()\n",
    "colors = ['#3498db', '#e74c3c']\n",
    "axes[0].bar(['Absence (0)', 'Presence (1)'], target_counts.values, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title('Heart Disease Class Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(target_counts.values):\n",
    "    axes[0].text(i, v + 5, str(v), ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "target_pct = df['Target'].value_counts(normalize=True) * 100\n",
    "labels = [f'Absence\\n({target_pct[0]:.1f}%)', f'Presence\\n({target_pct[1]:.1f}%)']\n",
    "axes[1].pie(target_counts.values, labels=labels, colors=colors, autopct='%1.1f%%', \n",
    "            startangle=90, explode=(0.05, 0.05), shadow=True)\n",
    "axes[1].set_title('Heart Disease Prevalence', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Class Distribution Summary:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Absence (0):   {target_counts[0]} ({target_pct[0]:.2f}%)\")\n",
    "print(f\"Presence (1):  {target_counts[1]} ({target_pct[1]:.2f}%)\")\n",
    "print(f\"\\n✓ Dataset is relatively balanced (not severely imbalanced)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c14261e",
   "metadata": {},
   "source": [
    "## 1.6 Feature Selection\n",
    "\n",
    "We'll select at least 6 relevant clinical features for our logistic regression model. Based on medical literature and data exploration, we'll focus on:\n",
    "\n",
    "1. **Age** - Patient age (years)\n",
    "2. **Cholesterol** - Serum cholesterol (mg/dL)\n",
    "3. **BP** - Resting blood pressure (mm Hg)\n",
    "4. **Max HR** - Maximum heart rate achieved\n",
    "5. **ST depression** - ST depression induced by exercise\n",
    "6. **Number of vessels fluro** - Number of major vessels colored by fluoroscopy (0-3)\n",
    "\n",
    "Let's examine the correlation between these features and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdef9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling\n",
    "selected_features = ['Age', 'Cholesterol', 'BP', 'Max HR', 'ST depression', 'Number of vessels fluro']\n",
    "\n",
    "# Create a subset with selected features and target\n",
    "df_model = df[selected_features + ['Target']].copy()\n",
    "\n",
    "print(\"Selected Features:\")\n",
    "print(\"=\"*60)\n",
    "for i, feat in enumerate(selected_features, 1):\n",
    "    print(f\"{i}. {feat}\")\n",
    "print(f\"\\nTotal features: {len(selected_features)}\")\n",
    "print(f\"Model dataset shape: {df_model.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e9edfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "correlation_matrix = df_model.corr()\n",
    "\n",
    "# Visualize correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.3f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display correlations with target\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Correlation with Target (Heart Disease):\")\n",
    "print(\"=\"*60)\n",
    "target_corr = correlation_matrix['Target'].drop('Target').sort_values(ascending=False)\n",
    "for feat, corr in target_corr.items():\n",
    "    direction = \"↑ Positive\" if corr > 0 else \"↓ Negative\"\n",
    "    print(f\"{feat:30s}: {corr:7.3f}  {direction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f1ffc5",
   "metadata": {},
   "source": [
    "## 1.7 Train-Test Split (Stratified)\n",
    "\n",
    "We'll split the data into training (70%) and testing (30%) sets using **stratified sampling** to maintain the class distribution in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b27bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual stratified split (implementing from scratch, no sklearn for core logic)\n",
    "def stratified_train_test_split(X, y, test_size=0.3, random_state=42):\n",
    "    \"\"\"\n",
    "    Manually perform stratified train-test split.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: features (numpy array or pandas DataFrame)\n",
    "    - y: target (numpy array or pandas Series)\n",
    "    - test_size: proportion of test set (default 0.3)\n",
    "    - random_state: random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    - X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Convert to numpy if needed\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        X = X.values\n",
    "    if isinstance(y, pd.Series):\n",
    "        y = y.values\n",
    "    \n",
    "    # Get indices for each class\n",
    "    class_0_idx = np.where(y == 0)[0]\n",
    "    class_1_idx = np.where(y == 1)[0]\n",
    "    \n",
    "    # Shuffle indices\n",
    "    np.random.shuffle(class_0_idx)\n",
    "    np.random.shuffle(class_1_idx)\n",
    "    \n",
    "    # Calculate split points\n",
    "    n_test_0 = int(len(class_0_idx) * test_size)\n",
    "    n_test_1 = int(len(class_1_idx) * test_size)\n",
    "    \n",
    "    # Split indices\n",
    "    test_idx_0 = class_0_idx[:n_test_0]\n",
    "    train_idx_0 = class_0_idx[n_test_0:]\n",
    "    \n",
    "    test_idx_1 = class_1_idx[:n_test_1]\n",
    "    train_idx_1 = class_1_idx[n_test_1:]\n",
    "    \n",
    "    # Combine indices\n",
    "    train_idx = np.concatenate([train_idx_0, train_idx_1])\n",
    "    test_idx = np.concatenate([test_idx_0, test_idx_1])\n",
    "    \n",
    "    # Shuffle combined indices\n",
    "    np.random.shuffle(train_idx)\n",
    "    np.random.shuffle(test_idx)\n",
    "    \n",
    "    # Create splits\n",
    "    X_train = X[train_idx]\n",
    "    X_test = X[test_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_test = y[test_idx]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Prepare features and target\n",
    "X = df_model[selected_features].values\n",
    "y = df_model['Target'].values\n",
    "\n",
    "# Perform stratified split\n",
    "X_train, X_test, y_train, y_test = stratified_train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Display split information\n",
    "print(\"Train-Test Split Results:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training set:   {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set:       {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nFeatures: {X_train.shape[1]}\")\n",
    "\n",
    "# Verify stratification\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Class Distribution Verification:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original:  Absence={np.sum(y==0)} ({np.mean(y==0)*100:.1f}%), Presence={np.sum(y==1)} ({np.mean(y==1)*100:.1f}%)\")\n",
    "print(f\"Training:  Absence={np.sum(y_train==0)} ({np.mean(y_train==0)*100:.1f}%), Presence={np.sum(y_train==1)} ({np.mean(y_train==1)*100:.1f}%)\")\n",
    "print(f\"Test:      Absence={np.sum(y_test==0)} ({np.mean(y_test==0)*100:.1f}%), Presence={np.sum(y_test==1)} ({np.mean(y_test==1)*100:.1f}%)\")\n",
    "print(\"\\n✓ Stratification maintained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323bf4c5",
   "metadata": {},
   "source": [
    "## 1.8 Feature Normalization\n",
    "\n",
    "We'll normalize the numerical features using **standardization (z-score normalization)**:\n",
    "$$\n",
    "z = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "where $\\mu$ is the mean and $\\sigma$ is the standard deviation.\n",
    "\n",
    "**Important:** We fit the scaler on the training data only, then transform both train and test sets to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c72645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual standardization (z-score normalization)\n",
    "def standardize(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Standardize features using z-score normalization.\n",
    "    Fit on training data, transform both train and test.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: training features\n",
    "    - X_test: test features\n",
    "    \n",
    "    Returns:\n",
    "    - X_train_scaled, X_test_scaled, mean, std\n",
    "    \"\"\"\n",
    "    # Calculate mean and std from training data only\n",
    "    mean = np.mean(X_train, axis=0)\n",
    "    std = np.std(X_train, axis=0)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    std[std == 0] = 1.0\n",
    "    \n",
    "    # Transform both sets\n",
    "    X_train_scaled = (X_train - mean) / std\n",
    "    X_test_scaled = (X_test - mean) / std\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, mean, std\n",
    "\n",
    "# Apply standardization\n",
    "X_train_scaled, X_test_scaled, feature_mean, feature_std = standardize(X_train, X_test)\n",
    "\n",
    "print(\"Feature Normalization (Z-score Standardization):\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nStatistics (from training data):\")\n",
    "print(\"-\"*60)\n",
    "for i, feat in enumerate(selected_features):\n",
    "    print(f\"{feat:30s}: μ={feature_mean[i]:8.2f}, σ={feature_std[i]:8.2f}\")\n",
    "\n",
    "# Display sample of normalized data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Sample Normalized Values (first 3 training samples):\")\n",
    "print(\"=\"*60)\n",
    "sample_df = pd.DataFrame(X_train_scaled[:3], columns=selected_features)\n",
    "print(sample_df.to_string())\n",
    "\n",
    "# Verify normalization (training set should have mean≈0, std≈1)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Verification (Training Set After Normalization):\")\n",
    "print(\"=\"*60)\n",
    "train_mean = np.mean(X_train_scaled, axis=0)\n",
    "train_std = np.std(X_train_scaled, axis=0)\n",
    "for i, feat in enumerate(selected_features):\n",
    "    print(f\"{feat:30s}: μ≈{train_mean[i]:6.3f}, σ≈{train_std[i]:6.3f}\")\n",
    "print(\"\\n✓ Features successfully normalized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74e04c4",
   "metadata": {},
   "source": [
    "## Step 1 Summary: Data Insights\n",
    "\n",
    "### Dataset Overview\n",
    "- **Source:** Kaggle Heart Disease Dataset (https://www.kaggle.com/datasets/neurocipher/heartdisease)\n",
    "- **Total Samples:** 303 patient records\n",
    "- **Features:** 14 clinical features (selected 6 for modeling)\n",
    "- **Target:** Binary classification (0 = Absence, 1 = Presence)\n",
    "\n",
    "### Key Findings from EDA\n",
    "\n",
    "1. **Class Distribution:**\n",
    "   - Disease Absence: ~45% of samples\n",
    "   - Disease Presence: ~55% of samples\n",
    "   - ✓ Dataset is relatively balanced\n",
    "\n",
    "2. **Data Quality:**\n",
    "   - ✓ No missing values\n",
    "   - Some outliers detected in clinical features (kept as valid extreme cases)\n",
    "\n",
    "3. **Selected Features:**\n",
    "   - Age (29-77 years)\n",
    "   - Cholesterol (126-564 mg/dL)\n",
    "   - Blood Pressure (94-200 mm Hg)\n",
    "   - Max Heart Rate (71-202 bpm)\n",
    "   - ST Depression (0.0-6.2)\n",
    "   - Number of Vessels (0-3)\n",
    "\n",
    "4. **Feature Correlations with Target:**\n",
    "   - Strong positive correlations: Number of vessels, ST depression\n",
    "   - Moderate negative correlation: Max HR\n",
    "   - Weak correlations: Age, Cholesterol, BP\n",
    "\n",
    "### Data Preprocessing Completed\n",
    "\n",
    "✓ **Binarization:** Target variable converted to 0/1  \n",
    "✓ **Train-Test Split:** 70% training (212 samples), 30% test (91 samples), stratified  \n",
    "✓ **Normalization:** Z-score standardization applied (fit on train, transform both)\n",
    "\n",
    "### Ready for Step 2: Logistic Regression Implementation\n",
    "\n",
    "The data is now clean, preprocessed, and ready for model training!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
